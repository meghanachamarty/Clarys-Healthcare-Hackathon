# -*- coding: utf-8 -*-
"""Autoimmunes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bFdRhnC0CnB10JT4EzKYsGBFBvKuY7bP

# CNN
"""

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from io import BytesIO
import requests
from zipfile import ZipFile

# Function to load images and labels from URL
def load_images_and_labels_from_url(zip_url):
    response = requests.get(zip_url)
    zip_file = ZipFile(BytesIO(response.content))

    images = []
    labels = []

    for image_file in zip_file.namelist():
        if image_file.endswith(".jpg"):
            with zip_file.open(image_file) as file:
                image_data = file.read()
                image = cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR)
                label = image_file.split('_')[1]
                labels.append(label)
                images.append(image)

    return np.array(images), np.array(labels)

# Specify the URL of the HAM10000 dataset (replace with the actual URL)
ham10000_zip_url = 'https://github.com/RRazor/ham10000/raw/main/HAM10000.zip'

# Load images and labels from the URL
images, labels = load_images_and_labels_from_url(ham10000_zip_url)

# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

# Convert labels to one-hot encoding
labels_one_hot = to_categorical(labels_encoded)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels_one_hot, test_size=0.2, random_state=42)

# Specify the dimensions of your input images
img_height, img_width, num_channels = 64, 64, 3  # Adjust these values based on your images
num_classes = len(np.unique(labels_encoded))

# Create the CNN model
model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, num_channels)))
model.add(layers.MaxPooling2D((2, 2)))

# Add more convolutional and pooling layers as needed

model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical
import requests
from io import BytesIO

# Download HAM10000 metadata file
metadata_url = "https://dataverse.harvard.edu/api/access/datafile/3046685?format=original&gbrecs=true"
metadata_response = requests.get(metadata_url)
metadata_content = BytesIO(metadata_response.content)
metadata = pd.read_csv(metadata_content)

# Load and preprocess images
image_size = (224, 224)  # Adjust according to your model's input size
images = []
labels = []

for index, row in metadata.iterrows():
    img_url = "https://dataverse.harvard.edu/api/access/datafile/" + str(row["image_id"]) + "?format=original"
    img_response = requests.get(img_url)
    img = load_img(BytesIO(img_response.content), target_size=image_size)
    img_array = img_to_array(img)
    images.append(img_array)
    labels.append(row["dx"])

# Convert lists to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Encode labels to integers
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)
num_classes = len(label_encoder.classes_)

# Convert labels to one-hot encoding
one_hot_labels = to_categorical(encoded_labels, num_classes=num_classes)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    images, one_hot_labels, test_size=0.2, random_state=42
)

# Calculate class weights to handle class imbalance
class_weights = class_weight.compute_class_weight('balanced', np.unique(labels), labels)
class_weights_dict = dict(enumerate(class_weights))

# Build the CNN model
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, class_weight=class_weights_dict)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc}')

!git clone https://github.com/deveshsangwan/Skin-lesions-classification.git

# Commented out IPython magic to ensure Python compatibility.
# %cd Skin-lesions-classification

!ls

# Commented out IPython magic to ensure Python compatibility.
# %load VGG_skin_lesions.ipynb



"""# New Section"""

pip install tensorflow tensorflow gpu opencv python matplotlib

pip list

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Load metadata.csv
metadata_path = 'metadata.csv'
metadata_df = pd.read_csv(metadata_path)

# Path to training and testing data
train_data_dir = 'training'
test_data_dir = 'testing'

# Parameters for the CNN model and training
input_shape = (150, 150, 3)
num_classes = len(metadata_df['type'].unique())
batch_size = 32
epochs = 10

# Preprocess and augment the training images
train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(150, 150), batch_size=batch_size, class_mode='categorical')

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=epochs)

# Load testing images
test_images = []
test_filenames = []

for filename in os.listdir(test_data_dir):
    if filename.endswith('.jpg') or filename.endswith('.png'):
        img_path = os.path.join(test_data_dir, filename)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150))
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = tf.expand_dims(img_array, 0)
        test_images.append(img_array)
        test_filenames.append(filename)

test_images = np.vstack(test_images)

# Make predictions
predictions = model.predict(test_images)

# Get predicted classes
predicted_classes = np.argmax(predictions, axis=1)

# Map predicted classes to category names
category_mapping = {index: category for index, category in enumerate(metadata_df['type'].unique())}

# Output predictions
for filename, predicted_class in zip(test_filenames, predicted_classes):
    category = category_mapping[predicted_class]
    print(f"File: {filename}, Predicted Category: {category}")